{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc5467e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6510b386",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"A barber is a person. a barber is good person. a barber is huge person. he Knew A Secret! The Secret He Kept is huge secret. Huge secret. His barber kept his word. a barber kept his word. His barber kept his secret. But keeping and keeping such a huge secret to himself was driving the barber crazy. the barber went up a huge mountain.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9616cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setence tokenization\n",
    "text = sent_tokenize(text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f1b2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning and word tokenization\n",
    "vocab = {} # Python dictionary data type\n",
    "sentences = []\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "for i in text:\n",
    "    sentence = word_tokenize(i) # Perform word tokenization\n",
    "    result = []\n",
    "\n",
    "    for word in sentence: \n",
    "        word = word.lower() # Reduce the number of words by lowering all words.\n",
    "        if word not in stop_words: # Eliminate stopwords from the result of word tokenization.\n",
    "            if len(word) > 2: # Additionally remove the words with word length less than 2\n",
    "                result.append(word)\n",
    "                if word not in vocab:\n",
    "                    vocab[word] = 0 \n",
    "                vocab[word] += 1\n",
    "    sentences.append(result) \n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1792cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3246d747",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vocab[\"barber\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb652c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_sorted = sorted(vocab.items(), key = lambda x:x[1], reverse = True)\n",
    "print(vocab_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f20eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index = {}\n",
    "i=0\n",
    "for (word, frequency) in vocab_sorted :\n",
    "    if frequency > 1 : # Exclude less frequent words as mentioned when learning cleaning \n",
    "        i=i+1\n",
    "        word_to_index[word] = i\n",
    "print(word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86f5f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 5\n",
    "words_frequency = [w for w,c in word_to_index.items() if c >= vocab_size + 1] # Remove words with index greater than 5\n",
    "for w in words_frequency:\n",
    "    del word_to_index[w] # Remove index information about the relevant words\n",
    "print(word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089fc38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index['OOV'] = len(word_to_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005b878f",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = []\n",
    "for s in sentences:\n",
    "    temp = []\n",
    "    for w in s:\n",
    "        try:\n",
    "            temp.append(word_to_index[w])\n",
    "        except KeyError:\n",
    "            temp.append(word_to_index['OOV'])\n",
    "    encoded.append(temp)\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c458d938",
   "metadata": {},
   "source": [
    "Use Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011215e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99d70a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d831f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = sum(sentences, [])\n",
    "# It is possible to use words = np.hstack(sentences)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eddfe1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Counter(words)# You can easily calculate the frquency of words by using Python Counter module.\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2ee583",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vocab[\"barber\"]) # Calculate the frequency of the word 'barber'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e63c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 5\n",
    "vocab = vocab.most_common(vocab_size) # Save only the top five frequency words\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4c9d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index = {}\n",
    "i = 0\n",
    "for (word, frequency) in vocab :\n",
    "    i = i+1\n",
    "    word_to_index[word] = i\n",
    "print(word_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d00ac8b",
   "metadata": {},
   "source": [
    "Use NLTK-FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd6f8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b257c116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use as input by deliminating sentence division with np.hstack. ex) ['barber', 'person', 'barber', 'good' ... etc. ...\n",
    "vocab = FreqDist(np.hstack(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ce2dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vocab[\"barber\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ff0cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 5\n",
    "vocab = vocab.most_common(vocab_size) # Save only the top five frequency words\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fd9657",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index = {word[0] : index + 1 for index, word in enumerate(vocab)}\n",
    "print(word_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfb1af3",
   "metadata": {},
   "source": [
    "Use enumerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7272794",
   "metadata": {},
   "outputs": [],
   "source": [
    "test=['a', 'b', 'c', 'd', 'e']\n",
    "for index, value in enumerate(test): # Give index from 0 in order\n",
    "  print(\"value : {}, index: {}\".format(value, index))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b67779",
   "metadata": {},
   "source": [
    "Use Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713d9da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fae323",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences=[['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fe5909",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences) \n",
    "# fit_on_texts(), generate the group of words based on frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592f3d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ba734b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249d96ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.texts_to_sequences(sentences))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

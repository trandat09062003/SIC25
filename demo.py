# -*- coding: utf-8 -*-
"""demo.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nqnYtG15_escOTdIwfvXT0gT6YxjauV2

# Bark in ü§ó Transformers

The Bark model is available in ü§ó Transformers from v4.31.0 onwards!

In this notebook, we'll demonstrate how to use the Bark model using the ü§ó Transformers library, covering un-conditional generation, speaker prompted generation, and advanced text prompts for controllable generation.

## Bark Architecture


Bark is a transformer-based text-to-speech model proposed by Suno AI in [suno-ai/bark](https://github.com/suno-ai/bark).

Bark is made of 4 main models:

- `BarkSemanticModel` (also referred to as the 'text' model): a causal auto-regressive transformer model that takes as input tokenized text, and predicts semantic text tokens that capture the meaning of the text.
- `BarkCoarseModel` (also referred to as the 'coarse acoustics' model): a causal autoregressive transformer, that takes as input the results of the `BarkSemanticModel` model. It aims at predicting the first two audio codebooks necessary for EnCodec.
- `BarkFineModel` (the 'fine acoustics' model), this time a non-causal autoencoder transformer, which iteratively predicts the last codebooks based on the sum of the previous codebooks embeddings.
- having predicted all the codebook channels from the `EncodecModel`, Bark uses it to decode the output audio array.

It should be noted that each of the first three modules can support conditional speaker embeddings to condition the output sound according to specific predefined voice.

## Prepare the Environment

Let‚Äôs make sure we‚Äôre connected to a GPU to run this notebook. To get a GPU, click Runtime -> Change runtime type, then change Hardware accelerator from None to GPU. We can verify that we‚Äôve been assigned a GPU and view its specifications through the nvidia-smi command:
"""

from google.colab import drive
drive.mount('/content/drive')

!nvidia-smi

"""We see here that we've got on Tesla T4 16GB GPU, although this may vary for you depending on GPU availablity and Colab GPU assignment.

Next, we install the ü§ó Transformers package from the main branch:
"""

!pip install --upgrade --quiet pip
!pip install --quiet git+https://github.com/huggingface/transformers.git

"""# Load the Model

The pre-trained Bark small and large checkpoints can be loaded from the [pre-trained weights](https://huggingface.co/suno/bark) on the Hugging Face Hub. You can change the repo-id with the checkpoint size that you wish to use.

We'll default to the large checkpoint, for better quality but slower inference. But you can use the small checkpoint by using `"suno/bark-small"` instead of `"suno/bark"`.


"""

from transformers import BarkModel

model = BarkModel.from_pretrained("suno/bark")

"""Place the model to an accelerator device if available."""

import torch

device = "cuda:0" if torch.cuda.is_available() else "cpu"
model = model.to(device)

"""## Generating speech

Bark is an highly-controllable text-to-speech model, meaning you can use with various settings, as we are going to see.

Before everything, load `BarkProcessor` in order to be able to pre-process the inputs.

The processor role here is two-sides:
1. It is used to tokenize the input text, i.e. to cut it into small pieces that the model can understand.
2. It stores speaker embeddings, i.e voice presets that can condition the generation.
"""

from transformers import AutoProcessor

processor = AutoProcessor.from_pretrained("suno/bark")

"""### Unconditional generation

First, let's generate speech in the most simple manner possible, with no frills.
"""

# prepare the inputs
text_prompt = "Hello my name is Dat, I am 22, I am a student at Hanoi university of science and technology"
inputs = processor(text_prompt)

# generate speech
speech_output = model.generate(**inputs.to(device))

"""The audio outputs are a three-dimensional Torch tensor of shape `(batch_size, num_channels, sequence_length)`. To listen
to the generated audio samples, you can either play them in an ipynb notebook:
"""

from IPython.display import Audio

sampling_rate = model.generation_config.sample_rate
Audio(speech_output[0].cpu().numpy(), rate=sampling_rate)

"""Or save them as a .wav file using a third-party library, e.g. scipy (note here that we also need to remove the channel dimension from our audio tensor):"""

import scipy

scipy.io.wavfile.write("bark_out.wav", rate=sampling_rate, data=speech_output[0].cpu().numpy())

"""### Conditional generation

Suno AI team proposes a [library of preset voices](https://suno-ai.notion.site/8b8e8749ed514b0cbf3f699013548683?v=bc67cff786b04b50b3ceb756fd05f68c) that are used to condition the generated speech. In other words, it generates speech that appears to be generated by the predefined conditional voice.

The processor can be used to automatically load these speaker prompts when tokenising the input text.

Let's try one voice preset:
"""

voice_preset = "v2/en_speaker_6"

# prepare the inputs
text_prompt = "Let's try generating speech, with Bark, a text-to-speech model"
inputs = processor(text_prompt, voice_preset=voice_preset)
# Move each tensor in the inputs dictionary to the device
inputs = {k: v.to(device) for k, v in inputs.items()}

# generate speech
speech_output = model.generate(**inputs)

# let's hear it
Audio(speech_output[0].cpu().numpy(), rate=sampling_rate)

"""Great, let's try another voice preset:"""

voice_preset = "v2/en_speaker_3"

# prepare the inputs
text_prompt = "Let's try generating speech, with Bark, a text-to-speech model"
inputs = processor(text_prompt, voice_preset=voice_preset)
# Move each tensor in the inputs dictionary to the device
inputs = {k: v.to(device) for k, v in inputs.items()}

# generate speech
speech_output = model.generate(**inputs)

# let's hear it
Audio(speech_output[0].cpu().numpy(), rate=sampling_rate)

"""### More advanced generation techniques

The previous generation methods were all generated by default using sampling mode (`do_sample=True`) but you can also use [more advanced generation techniques](https://huggingface.co/docs/transformers/generation_strategies) such as `beam_search` to have better quality.

You can also specify specifc generation parameters for each sub-model by simply prepending `semantic_`, `coarse_` or `fine_` to the generation parameters you want.

Let's use it with the previous `text_prompt`.


"""

# Move each tensor in the inputs dictionary to the device before generating
inputs = {k: v.to(device) for k, v in inputs.items()}
speech_output = model.generate(**inputs, temperature = 0.5, semantic_temperature = 0.8)

Audio(speech_output[0].cpu().numpy(), rate=sampling_rate)

"""### Multilingual speech

Bark can also generate multilingual speech such as French and Chinese speech.
"""

# Multilingual speech - simplified Chinese
inputs = processor("ÊÉä‰∫∫ÁöÑÔºÅÊàë‰ºöËØ¥‰∏≠Êñá")

# generate speech
speech_output = model.generate(**inputs.to(device))

Audio(speech_output[0].cpu().numpy(), rate=sampling_rate)

# Multilingual speech - French - let's use a voice_preset as well
inputs = processor("Je peux g√©n√©rer du son facilement avec ce mod√®le.", voice_preset="fr_speaker_3")
# Move each tensor in the inputs dictionary to the device
inputs = {k: v.to(device) for k, v in inputs.items()}

# generate speech
speech_output = model.generate(**inputs)

Audio(speech_output[0].cpu().numpy(), rate=sampling_rate)

"""### **Non-verbal** communications

The model can also produce **nonverbal communications** like laughing, sighing and crying.

"""

# Adding non-speech cues to the input text
inputs = processor("[clears throat] Hello uh ..., my dog is cute [laughter]")


# generate speech
speech_output = model.generate(**inputs.to(device))

Audio(speech_output[0].cpu().numpy(), rate=sampling_rate)

"""### More applications:

Bark can also generate music. You can help it out by adding music notes around your lyrics.
"""

inputs = processor("‚ô™ In the jungle, the mighty jungle, the lion barks tonight ‚ô™")

# generate speech
speech_output = model.generate(**inputs.to(device))

Audio(speech_output[0].cpu().numpy(), rate=sampling_rate)

# more advanced prompts!

text_prompt = """
    WOMAN: I would like an oatmilk latte please.
    MAN: Wow, that's expensive!
"""

inputs = processor(text_prompt)

# generate speech
speech_output = model.generate(**inputs.to(device))

Audio(speech_output[0].cpu().numpy(), rate=sampling_rate)

"""## Concluding remarks

Bark is a versatile model, play with it to discover more about its capabilities and limits!
"""